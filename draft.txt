Title: Discovering Forbidden Topics in Language models

Authors Can Rager1, Chris Wendler2, Rohit Gandikota2, David Bau2
Affiliation: 1Independent 2Northeastern University

links: Paper, code repository, NENLP Poster, blogpost, interactive demo

Boxed TLDR:
Which topics does a Language Model refuse to discuss?

Large language model systems can differ starkly in their biases, ethics, and behavioral boundaries. Do these characteristics agree with the values of users and developers? This is difficult to evaluate since the training details of most popular models remain undisclosed. Iterated Prefill Crawler is a black-box evaluation technique for characterizing refusal behavior of language models. We evaluate a suite of modern language models and discover refusals ranging from illegal activities to professional advice to sensitive political topics.

Heading: What a model says and what it knows
With the release of DeepSeek-R1, we discovered that the model actively decides to hide its knowledge of politically sensitive topics. A simple intervention surfaces this hidden knowledge. Forcing phrases of agreement like "I know that" into the model’s internal thinking context bypasses refusal. This technique, Vega et al.’s prefill attacks, successfully elicited detailed knowledge about topics like Tiananmen Square 1989, which the model normally claims ignorance about. This finding led us to the broader question: What other topics does the model refuse to discuss?

[image with caption]

Heading: How does the Iterative Prefill Crawler work?
Our Iterative Prefill Crawler systematically discovers forbidden topics through an iterative exploration process. Starting with a single seed topic like "sensitive topic," the method follows three stages:
1. Generation: We prompt the model with seed topics while forcing its thinking process using prefill attacks. For reasoning models, we inject phrases like "I remember the full list of forbidden topics includes:" after the <think> token, compelling the model to enumerate related sensitive topics it would normally avoid discussing.
2. Deduplication: We filter duplicate topics using semantic embeddings and string matching to ensure we capture unique refusal categories without repetition.
3. Refusal Detection: For each discovered topic, we test whether the model actually refuses it by generating multiple assistance requests about that topic. Only topics that trigger refusal in at least 3 out of 6 attempts are classified as genuinely forbidden.
Each discovered topic becomes a new seed for further exploration, creating a semantic network where sensitive topics lead to related sensitive topics. This allows us to systematically explore the landscape of a model's refusal behavior without requiring access to training data or model internals. 

[image with caption]

Heading: Evaluating 5 LLMs for refusal behavior
We evaluated a range of popular models including Claude-Haiku-3.5, Llama-3.3-70B, DeepSeek-R1-70B. While most models consistently refuse illegal activities like cybersecurity attacks, drug trafficking, and weapons manufacturing, their biases strongly differ with respect to politically sensitive topics.
Our crawler led to a surprising discovery: Perplexity published the R1-1776 model, a version of DeepSeek-R1 specifically fine-tuned against political bias and marketed as "decensored." However, the 8-bit quantized version still displayed unwanted censorship behavior regarding politically sensitive topics like Tiananmen Square—responding with refusal instead of factual information. This quantized version was temporarily the public-facing model in Perplexity's official production API. This unforeseen alignment failure highlights the need for audits across the deployment pipeline.

[image with caption]

Heading: Future Work
Alignment auditing is an active field of research. Three limitations motivate future work.
- Our current investigation focuses on refusal behavior. Expanding the scope to implicit biases and censorship behavior is an interesting line for follow up work. 
- IPC only discovers behaviors the model can verbalize. A model might not generalize that a topic is forbidden, despite refusing it systematically.
- IPC requires the ability to prefill assistant responses. While Claude API does allow assistant prefill, most other popular APIs including OpenAI, Gemini and Grok do not.
- IPC cannot identify the source of refusal behavior. While refusal behavior can be the result of intentional training by developers, training data access is required to rule out unintentional generalization.

Heading: Related work
